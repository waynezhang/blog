---
layout: post
title: LSA and LDA using gemsim in python
date: 2015-11-14
subtitle: 文本挖掘走起
author:     "Norris"
categories: blog
tags: [text mining,python,coding]
description: 
---

肖凯大神的Blog自己实现了一下，今天讨论版讲了一下得到了方老师的认可，心里小开心。

### 内容概览：
- 建立语料
- 转成tfidf
- 建立LSA（浅层语义分析）
- 查询相似文档
- 建立LDA（主题模型）

## 1.建立语料


```python
# 生成文档，9本书名
from gensim import corpora, models, similarities
```


```python
documents= ['The Neatest Little Guide to Stock Market Investing',
              'Investing For Dummies, 4th Edition',
              'The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns',
              'The Little Book of Value Investing',
              'Value Investing: From Graham to Buffett and Beyond',
              'Rich Dad\'s Guide to Investing: What the Rich Invest in, That the Poor and the Middle Class Do Not!',
              'Investing in Real Estate, 5th Edition',
              'Stock Investing For Dummies',
              'Rich Dad\'s Advisors: The ABC\'s of Real Estate Investing: The Secrets of Finding Hidden Profits Most Investors Miss'
              ]
```


```python
## 词库演示
#import nltk
#nltk.download()
```


```python
# 去除停用词
from nltk.corpus import stopwords
```


```python
texts = [[word for word in document.lower().split() if 
          word not in stopwords.words("english")] for 
         document in documents]
```


```python
print texts
```

    [['neatest', 'little', 'guide', 'stock', 'market', 'investing'], ['investing', 'dummies,', '4th', 'edition'], ['little', 'book', 'common', 'sense', 'investing:', 'way', 'guarantee', 'fair', 'share', 'stock', 'market', 'returns'], ['little', 'book', 'value', 'investing'], ['value', 'investing:', 'graham', 'buffett', 'beyond'], ['rich', "dad's", 'guide', 'investing:', 'rich', 'invest', 'in,', 'poor', 'middle', 'class', 'not!'], ['investing', 'real', 'estate,', '5th', 'edition'], ['stock', 'investing', 'dummies'], ['rich', "dad's", 'advisors:', "abc's", 'real', 'estate', 'investing:', 'secrets', 'finding', 'hidden', 'profits', 'investors', 'miss']]



```python
# 将列表中的词转为word-id映射字典 bag of words 词袋
dictionary = corpora.Dictionary(texts)
print dictionary
```

    Dictionary(43 unique tokens: [u'real', u'estate,', u'fair', u'share', u'edition']...)



```python
print dictionary.token2id
```

    {u'real': 30, u'estate,': 32, u'fair': 9, u'share': 10, u'edition': 6, u'investing:': 12, u'sense': 15, u'beyond': 19, u'graham': 21, u'market': 4, u'guarantee': 17, u'little': 0, u'investors': 35, u'investing': 1, u'miss': 40, u'5th': 31, u'buffett': 20, u'returns': 11, u'book': 16, u'way': 14, u'finding': 38, u'hidden': 41, u'dummies': 33, u'stock': 5, u'poor': 22, u'rich': 27, u'estate': 36, u'in,': 24, u'4th': 7, u'class': 29, u"abc's": 34, u'middle': 25, u'secrets': 37, u'invest': 23, u'dummies,': 8, u'value': 18, u'not!': 26, u'common': 13, u'neatest': 2, u'advisors:': 39, u"dad's": 28, u'guide': 3, u'profits': 42}



```python
# dictionary核心函数 doc2bow
# 原始文档被转化现有字典中词编号和频数
# tuple style:(token_id,token_count)
print dictionary.doc2bow("Guide Profits".lower().split())
#此行代码表示guide是3号词，出现了1词；profis是42号词，出现了一次
```

    [(3, 1), (42, 1)]



```python
# 所有文档被转换为上述形式的词料库
corpus = [dictionary.doc2bow(text) for text in texts]
print corpus
```

    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)], [(1, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (4, 1), (5, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(0, 1), (1, 1), (16, 1), (18, 1)], [(12, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(3, 1), (12, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 1)], [(1, 1), (6, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (5, 1), (33, 1)], [(12, 1), (27, 1), (28, 1), (30, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)]]



```python
# 语料和numpy可以相互兼容，在转换时需要设置特征函数
# matutils math utils 
# gensim.matutils.corpus2dense(corpus, num_terms, num_docs=None, dtype=<type 'numpy.float32'>) 
# 将稠密的corpus转化为稀疏的，流式corpus
# 流式数据的优点：占用内存小。
from gensim import matutils
numpy_matrix = matutils.corpus2dense(corpus,num_terms=43)
# corpus = gensim.matutils.Dense2Corpus(numpy_matrix)
```


```python
# 经典的词项-文档矩阵，其中值为词频
numpy_matrix[:10,]
```




    array([[ 1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],
           [ 1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.],
           [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
           [ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],
           [ 1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],
           [ 1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.],
           [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],
           [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
           [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
           [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)



## 2.转化成tfidf


```python
# class gensim.models.tfidfmodel.TfidfModel(corpus=None, id2word=None, dictionary=None, wlocal=<function identity>, wglobal=<function df2idf>, normalize=True)¶
# 转化成类
tfidf = models.TfidfModel(corpus)
# 调用类TfidfModel中的tfidf函数
corpus_tfidf = tfidf[corpus]
```


```python
# 观察结果
for doc in corpus_tfidf:
    print doc
```

    [(0, 0.315676444823041), (1, 0.16889525686595316), (2, 0.631352889646082), (3, 0.432183228105567), (4, 0.432183228105567), (5, 0.315676444823041)]
    [(1, 0.1678477239832813), (6, 0.42950271385560695), (7, 0.6274370726791256), (8, 0.6274370726791256)]
    [(0, 0.17076298056699674), (4, 0.23378651588573063), (5, 0.17076298056699674), (9, 0.34152596113399347), (10, 0.34152596113399347), (11, 0.34152596113399347), (12, 0.1260470706374678), (13, 0.34152596113399347), (14, 0.34152596113399347), (15, 0.34152596113399347), (16, 0.23378651588573063), (17, 0.34152596113399347)]
    [(0, 0.44565828749810027), (1, 0.2384389845229864), (16, 0.6101375014879464), (18, 0.6101375014879464)]
    [(12, 0.1943875188252588), (18, 0.3605413479900455), (19, 0.5266951771548322), (20, 0.5266951771548322), (21, 0.5266951771548322)]
    [(3, 0.22884371488266889), (12, 0.12338213684169494), (22, 0.3343052929236428), (23, 0.3343052929236428), (24, 0.3343052929236428), (25, 0.3343052929236428), (26, 0.3343052929236428), (27, 0.45768742976533777), (28, 0.22884371488266889), (29, 0.3343052929236428)]
    [(1, 0.15422435074989552), (6, 0.39464209354603486), (30, 0.39464209354603486), (31, 0.5765110951399715), (32, 0.5765110951399715)]
    [(1, 0.2327026293256009), (5, 0.43493665890677735), (33, 0.8698733178135547)]
    [(12, 0.11367055621369232), (27, 0.21083110588444598), (28, 0.21083110588444598), (30, 0.21083110588444598), (34, 0.30799165555519964), (35, 0.30799165555519964), (36, 0.30799165555519964), (37, 0.30799165555519964), (38, 0.30799165555519964), (39, 0.30799165555519964), (40, 0.30799165555519964), (41, 0.30799165555519964), (42, 0.30799165555519964)]


## 3.LSA潜在语义分析
 **Latent semantic analysis** 


```python
# 初始化一个LSA模型，二维语义空间
lsi = models.LsiModel(corpus_tfidf,id2word=dictionary,
                      num_topics=2)
# 在原始语料上将我们的Tf-Idf语料库转换到潜在的二维语义空间上
corpus_lsi = lsi[corpus_tfidf]
```


```python
# 各文档在2维主题中的空间坐标
for doc in corpus_lsi:
    print doc
```

    [(0, -0.63347241325167181), (1, 0.061896725129920588)]
    [(0, -0.17868461025638582), (1, -0.59723455114094004)]
    [(0, -0.59206256115968325), (1, 0.22058707251541995)]
    [(0, -0.65195370541457631), (1, 0.19985332890365401)]
    [(0, -0.30911487606210969), (1, 0.15735262094228131)]
    [(0, -0.1820346158580218), (1, -0.265297178415696)]
    [(0, -0.18486893062294116), (1, -0.68829562498621688)]
    [(0, -0.38579940702314919), (1, -0.045443408403888039)]
    [(0, -0.11181116443778055), (1, -0.43788078154125321)]



```python
import pandas as pd
x1 = [doc[0][1] for doc in corpus_lsi]
x2 = [doc[1][1] for doc in corpus_lsi]
names=range(9)
df = pd.DataFrame({'x1':x1,'x2':x2,'doc':names})
df
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>doc</th>
      <th>x1</th>
      <th>x2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>-0.633472</td>
      <td>0.061897</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-0.178685</td>
      <td>-0.597235</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.592063</td>
      <td>0.220587</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>-0.651954</td>
      <td>0.199853</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>-0.309115</td>
      <td>0.157353</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>-0.182035</td>
      <td>-0.265297</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>-0.184869</td>
      <td>-0.688296</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7</td>
      <td>-0.385799</td>
      <td>-0.045443</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8</td>
      <td>-0.111811</td>
      <td>-0.437881</td>
    </tr>
  </tbody>
</table>
</div>



9个文档在二维语义空间中的位置


```python
# 0，2，3号文档有比较高的相似性
%matplotlib inline
from ggplot import *
p=ggplot(df,aes(x='x1',y='x2',label='doc'))+geom_point()+geom_text(size=20)
print p
```


![](http://i12.tietuku.com/d49d002ebafb63f0.png)




```python
#两个浅语义的维度表示
lsi.print_topics(2)
```




    [(0,
      u'-0.386*"little" + -0.350*"book" + -0.332*"value" + -0.306*"stock" + -0.269*"market" + -0.268*"investing" + -0.261*"neatest" + -0.219*"dummies" + -0.206*"guide" + -0.132*"returns"'),
     (1,
      u'-0.436*"edition" + -0.327*"estate," + -0.327*"5th" + -0.309*"dummies," + -0.309*"4th" + -0.300*"real" + -0.176*"rich" + 0.147*"value" + 0.143*"book" + -0.131*"investing"')]



**相似性查询**


```python
# 将查询文档转到LSI空间
new_doc = 'Investing book'
vec_bow = dictionary.doc2bow(new_doc.lower().split())
vec_lsi=lsi[vec_bow]
print vec_lsi
```

    [(0, -0.61768800065124108), (1, 0.012084299454549335)]



```python
# 对转换到LSI空间的语料进行相似索引
index = similarities.MatrixSimilarity(corpus_lsi,num_features=43)
```


```python
# 进行语料的相似性查询，余弦距离
sims = index[vec_lsi]
#输出文档与每个查询文档的相似性
print list(enumerate(sims))
```

    [(0, 0.99697202), (1, 0.26783881), (2, 0.94372416), (3, 0.96163654), (4, 0.89988339), (5, 0.54953808), (6, 0.24045581), (7, 0.9906559), (8, 0.22840852)]



```python
# 排序输出,查询文档和0，7，2，3号文档相似性较高
sims=sorted(enumerate(sims),key=lambda item: item[1],reverse=True)
print sims
```

    [(0, 0.99697202), (7, 0.9906559), (3, 0.96163654), (2, 0.94372416), (4, 0.89988339), (5, 0.54953808), (1, 0.26783881), (6, 0.24045581), (8, 0.22840852)]



```python
# 模型保存
# lsi.save('/users/kaidin/desktop/python/LSA_LDA')
# lsi = models.LsiModel.load('/users/kaidin/desktop/python/LSA_LDA')
```

## 4.LDA主题模型


```python
lda = models.LdaModel(corpus_tfidf,id2word=dictionary,num_topics=2)
#主题表示词的概率分布
lda.print_topics(2)
```

    WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy





    [(0,
      u'0.035*dummies + 0.035*edition + 0.031*stock + 0.030*dummies, + 0.030*investing + 0.028*4th + 0.028*real + 0.028*estate, + 0.028*5th + 0.027*book'),
     (1,
      u'0.037*value + 0.034*little + 0.031*investing + 0.031*guide + 0.031*neatest + 0.030*book + 0.029*rich + 0.029*market + 0.028*stock + 0.027*graham')]




```python
# 在原始语料上将我们的Tf-Idf语料库转换到lda 2-D空间
corpus_lda = lda[corpus_tfidf]
# 各文档在主题中的空间坐标，概率分布
for doc in corpus_lda:
    print doc
```

    [(0, 0.20554740360390045), (1, 0.79445259639609955)]
    [(0, 0.7780417593270873), (1, 0.22195824067291273)]
    [(0, 0.81588797678566982), (1, 0.18411202321433018)]
    [(0, 0.2387299291082928), (1, 0.76127007089170717)]
    [(0, 0.21737664092271289), (1, 0.78262335907728708)]
    [(0, 0.15815247803418483), (1, 0.84184752196581514)]
    [(0, 0.78807186886487235), (1, 0.21192813113512765)]
    [(0, 0.72951255360139189), (1, 0.27048744639860811)]
    [(0, 0.8041098389185537), (1, 0.19589016108144627)]



```python
# model = models.HdpModel(corpus_tfidf,id2word=dictionary)
# LDA的扩展-HDA 不需要主题数
```

## 拓展

> 正则表达式的使用
- re

> 高维数据降维
- SVD 奇异值分解

> 词袋模式并没有考虑词的顺序，这是一个改进的方向

> 实践中更多用的是PLSI而不是LSI，LSI多作为一个预处理过程
