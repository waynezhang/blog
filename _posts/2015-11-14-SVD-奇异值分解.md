---
layout: post
title: SVD奇异值分解
date: 2015-11-14
subtitle: 只知道特征值分解？那可不行呦
author:     "Norris"
categories: blog
tags: [text mining]
description: 本文错误较多，理解内涵就好。
---

近些天为了准备讨论班的演讲内容也是找了很多资料，最终定来下讲一下LDA、LSA，一来比较符合我学Python和做舆情项目的主题，二来我也很想体验一下python强大的文字处理能力。

## SVD分解

SVD奇异值分解。在处理词频矩阵的时候，经常因为数据量过大而难以下手（尽快学习spark），这时候就需要降维技术，SVD就是降维技术的一种，针对文本处理问题。也是LSA、LDA的基础步骤。

## 内容概览
- 基础知识补充
- SVD矩阵分解
- 低阶近似

注意：本文讨论的矩阵都是实数矩阵

## 一.基础知识补充

1. 特征根、特征向量
2. 特征值和矩阵的关系：考虑以下矩阵
	$$ \left(\begin{array}{ccc}
	30&0&0\\
	0&20&0\\
	0&0&1 \end{array} \right) $$
	该矩阵的特征值：
	$$\lambda_1=30,\lambda_2=20,\lambda_3=1$$ 
	对应的特征向量：

	$$\vec{x_1}=\left(\begin{array}{c}1\\0\\0\end{array}\right)\vec{x_1}=\left(\begin{array}{c}0\\1\\0\end{array}\right)
	\vec{x_1}=\left(\begin{array}{c}0\\0\\1\end{array}\right)$$

	假设 $$V^T=\left(\begin{array}{c}2\\4\\6\end{array}\right)$$,计算$$S \times V^T$$:

	$$ \begin{align}
	S\vec{V}&=S\left(2\vec{x_1}+4\vec{x_2}+6\vec{x_3}\right)\\&=2\lambda_1\vec{x_1}+4\lambda_2\vec{x_2}+6\lambda_3\vec{x_3}\\&=60\vec{x_1}+80\vec{x_2}+6\vec{x_3}\end{align}$$

	从上面的结果可以看出，矩阵与向量的相乘结果与特征值，特征向量有关。
	
	观察三个特征值λ1=30,λ2=20,λ3=1，λ3值最小，对计算结果的影响也最小，如果忽略λ3，那么运算结果就相当于从(60,80,6)转变为(60,80,0),这两个向量十分相近。这也表明了数值小得特征值对矩阵-向量相乘结果贡献小，影响小。我也是后面谈到的低阶近似的数学基础。

## 二.矩阵分解
1. 方阵的分解

	- 设S是$$M\times M$$方阵，则存在以下矩阵分解：

		$$S=U\Lambda U^{-1}$$
		
		其中U的列为S的特征向量，$$\Lambda$$为对角矩阵，其中对角线上的值为S的特征值，按从大到小排列：
			    
		$$ \left(\begin{array}{cccc}
		&\lambda_1\\
		&&\lambda_2\\
		&&&\ddots\\
		&&&&\lambda_M\end{array}\right),\lambda_i\geq\lambda_{i+1}$$
		
	- 设S是$$M\times M$$方阵，并且是对阵矩阵，有M个特征向量。则存在以下分解：
	
		$$S=Q\Lambda Q^T$$
	
		其中Q的列为矩阵S的单位蒸饺特征向量，$$\Lambda$$仍表示对角矩阵，其中对角线上的值为S的特征值，按从大到小排列。最后，$$Q^T=Q^{-1}$$,因为正交矩阵的逆等于其转置。

2. 奇异值分解
	上面讨论了方阵的分解，在LSA中，我们要对Term-Document矩阵进行分解，很显然这个矩阵不是方阵。这时需要奇异值分解对Term-Doucument进行分解。奇异值分解的推理使用到了上面所讲到的方阵分解。<br><br>
假设C是$$M\times N$$矩阵，U是$$M\times M$$矩阵，其中U的列为$$CC^T$$的正交特征向量，V为$$N\times N$$矩阵，其中V的列为$$C^TC$$的正交向量，再假设r为C矩阵的秩，则存在奇异值分解：

	$$C=U\Sigma V^T$$

	其中$$CC^T$$和$$C^TC$$的特征值相同，为$$\lambda_1,\ldots,\lambda_r$$<br>
	<br>
	$$\Sigma$$为$$M\times N$$矩阵，其中$$\Sigma_{ii}=\sqrt{\lambda_i}$$,其余位置数值为0，$$\Sigma_{ii}$$的值按大小降序排序。以下为$$\Sigma$$的完整数学定义：

	> For $$1\leq i \leq r$$,let $$\sigma_i=\sqrt{\lambda_i}$$,with $$\lambda_i\geq \lambda_{i+1}$$,Then the $$M\times N$$matrix$$\Sigma$$ is composed by setting $$\Sigma_{ii}=\sigma_i$$ for $$1\leq i\leq r$$, and zero otherwise	

	<br>
	$$\sigma_i$$称为矩阵C的奇异值。
	<br>
	C乘以其转置矩阵$$C^T$$得：
	
	$$CC^T=U\Sigma V^TV\Sigma U^T=U\Sigma^2U^T$$
	
	<br>
	上式正是在上节中讨论过得对称矩阵的分解。
	<br>
	奇异值分解的图形表示：
	<br>
	![奇异值图解](http://i12.tietuku.com/bbc2ee9231af7eef.jpg)
	<br>
	从图中可以看出$$\Sigma$$虽然为$$M\times N$$矩阵，但从第N+1行到M行全为0，因此可以表示成$$N\times N$$矩阵，又由于右式为矩阵乘机，因此U可以表示成$$M\times N$$矩阵，$$V^T$$可以表示成$$N\times N$$矩阵

## 三.低阶近似
LSA潜在语义分析中，低阶近似是为了使用低维的矩阵来表示一个高维的矩阵，并使两者之差尽可能的小。本节讨论低阶近似和F-范数。

给定一个$$M\times N$$矩阵 C(其秩为r)和正整数k，我们希望找到一个$$M\times N$$矩阵$$C_k$$,其秩不大于K。设X为C与$$C_k$$之间的差，X=C-$$C_k$$，X的F-范数为

$$\lVert X\rVert_F=\sqrt{\Sigma^M_{i=1}\Sigma^N_{j=1}X^2_{ij}}$$

当k远小于r时，称$$C_k$$为C的低阶近似，其中X也就是两矩阵之差的F-范数要尽可能小。

SVD可以被用于求低阶近似问题，步骤如下：

1. 给定一个矩阵C，对其进行奇异值分解：$$C=U\Sigma V^T$$
2. 构造$$\Sigma_k$$,它是将$$\Sigma$$的第k+1行至M行设为0，也就是把$$\Sigma$$的最小的r-k个奇异值设为0.
3. 计算$$C_k$$: $$C_k=U\Sigma_kV^T$$

回忆在基础知识一节里面曾经讲过，特征值数值的大小对矩阵-向量相乘影响的大小成正比，而奇异值和低特征也是正比关系，因此在这里选取数值最小的r-k个特征值设为0合乎情理，即我们所希望的$$C-C_k$$尽可能的小。

总结以上内容我们知道LSA的基本思路：LSA希望通过降低传统向量空间的维度来去除空间中的“噪音”，而降维可以通过SVD实现，因此首先对Term-Document矩阵进行SVD分解，然后降维并构造语义空间。
