---
layout: post
title: 再看数据挖掘--聚类
subtitle: 本系列属原创 转载请注明原著
date: 2016-03-17
author:     "Norris"
categories: blog
tags: [数据挖掘]
---

前两天百度面试刚问了K-means，就把以前多元学习的时候学到的皮毛说了一下，后悔没有早点再复习一下，亡羊补牢为时不晚，来整理整理思路说说聚类吧。

# 1.聚类概述

聚类是什么做什么的呢？

简单通俗的说就是将数据分成有意义或有用的组（簇），能够反映出数据的自然结构。聚类很多情况下都市其他问题的一个起点、或者说是一种准备工作。生物学中的界门纲目科属种就是一种层次化聚类，将杂乱无章的客户信息分成不同的组以实施不同的促销手段都是聚类的应用。聚类的作用大体来说分为：

1. 汇总。常见的汇总比方说PCA、SVD，但是面对数据特别复杂的时候是无法发挥作用的，所以需要简便的工具，例如聚类方法。

2. 压缩。这个技术常常用于图像、声音、视频的处理，他们数据特点都是：数据之间相似度较高；某些信息的丢失是可以接受的；希望发幅度压缩数据。

3. 有效发现最邻近。先找到邻近的簇，然后再从邻近的簇中选择邻近的点。

前面通俗的说了聚类的定义，具体来说就是仅根据在数据中发现的描述对象以及其关系信息，将数据对象分组。目标就是将使得组内的相似性很大，而组间的相似性很小。

聚类的类型是多种多样的，有一些定义需要区分：

1. 层次的与划分的。

	划分聚类，即簇的集合是非嵌套的，每个数据恰好在一个组中，例如K-means。层次聚类的簇集合是嵌套的，组成一个树（或者叫谱系图？），层次聚类可以看做划分聚类的序列，划分聚类可以通过取特定层得到。

2. 互斥的、重叠与模糊的。

	互斥，即每个对象只能指派到一个簇中。而重叠和模糊相反。重叠的现实意义是存在的，例如一个人可能是老师也可能是学生，因此需要将他指派到两个簇中。而模糊就是数学上模糊集的概念，每个数据对象不是一定属于或者不属于某个簇，而是以一定的概率分配给每个簇，但是实践中通过将对象指派到概率或者说隶属权值最高的簇中，因此模糊聚类也就转化成了互斥聚类。

3. 完全的与部分的。

	完全聚类，是指每个对象都能指派到一个簇中，而部分聚类相反。部分聚类的存在是因为为了舍弃一些噪声点、离群点或不感兴趣的点。

同时簇的类型也是多样的，不同的聚类方式使用的簇的类型也是不同的。粗略来说簇的类型大致可以分为：明显分离的，基于原型的，基于图的（连通分支、基于邻近的簇），基于密度的，共同性质的（概念簇）。

# 2.K-means

Okay，废话了这么多，我们就来说说具体的方法吧，先从最熟悉的K-means说起。

K均值算法可以大概描述为以下几个步骤：

1. 选择K个初始质心，K是需要人工选择的一个参数，就是所期望的簇的个数。

2. 每个点指派到最近的质心，指派到同一质心的点形成同一个簇。

3. 对上一步形成的每个簇计算新的质心。

4. 重复第2、第3步，直到簇或者质心不发生变化。

K均值总是会收敛到一个解，这是不同担心的，而且收敛通常发成在早期阶段，所以放宽一下第4步的要求，例如只有百分之一的点发成变化即可。

下面我们将每一步拆开详细的来说。

## 2.1 选择初始质心

K均值最大的一个问题，就是初始质心的选择会导致不一样的结果，而且某些结果效果可能会非常差。常见的方法就是随机的选择质心，这种方法就不是很好的一个选择。通常的处理方法也很简单：多运行几次，每次随机选取不同的初始质心，然后最后选择具有最小SSE的簇。这个方法虽然简单，但是效果可能一般，取决于我们的数据集以及簇的个数。

那么我们可以通过使用层次化聚类对他进行改进，首先对样本进行层次化聚类，然后提取K个簇，并将这些簇的质心最为初始质心。这种方法通常很有效，可是感觉很罗嗦有没有？他只能应用于样本较小，而且K相对于样本大小较小的情况。层次化聚类已经很繁琐了，而且层次化聚类已有的结果拿来再聚类，感觉不怎么实用。

另一种方式是首先随机的选取一个点（或者直接选择样本的均值作为质心），然后对每个后续的初始质心，选择离选取过的质心最远的点。这样就确保了我们的初始质心是散开的，但是有一个问题是这样很可能选取到离群点，而且计算距离最远点的开销也很大。

还有一种处理方式是K-means的改进，二分K-means方法。

还还有一种方法是使用后处理来修补所产生的簇集。

这两种方法放到后面介绍。

## 2.2 指派点到最近的质心

对于欧式空间中的点，我们通常使用欧几里得距离（$$L_2$$）计算，对于文档通常使用余弦相似性计算，通常还有例如曼哈顿距离（$$L_1$$）和Jaccard度量。

K均值的相似性度量需要比较简单的相似性度量，因为算法每一步都要重复的计算每个点的与质心的相似度。还有例如二分K均值通过减少相似度计算量来加快K均值速度。

## 2.3目标函数以及更新质心

聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或者点到质心的邻近性。对于欧几里得空间中的数据，我们常常使用误差平方和（SSE）来作为度量聚类质量的目标函数。SSE也成为散布。使得SSE最小的质心是均值。对于文档数据通常使用余弦相似度，也称作簇的凝聚度。其最优解也是均值。而曼哈顿距离作为度量的话，最优质心是中位数。而上面说到的欧几里得距离以及余弦相似度都可以看作是Bregman散度的特例，Bregman散度的最优解都是均值。

## 2.4其他问题

1. 空簇问题

	K均值存在的问题之一就是空簇的问题，就是说可能所有点都没有指派到某个簇内。这种情况下，通常会选择一个替补质心，否则平方误差会加大。一种方法是选择一个距离当前任何质心最远的店，后者从具有最大SSE簇中选择一个替补质心。

2. 离群点

	使用SSE标准的时候，离群点可能会过度影响所发现的簇，所得到的质心可能没有那么有代表性，提前删除他们是比较有用的方法。当然某些情况下是不能删除利群点的，因为有些时候离群点可能是人们感兴趣的点。

	那么如何识别离群点呢？提前删除是非常好的结果，如何提前检测在[后来的章节会说到](www.baidu.com)。还有一种方法是使用后处理的方法，例如可以记录每个点对SSE的影响，删除那些具有异乎寻常的影响点，此外还可以删除那些很小的簇，因为他们常常代表离群点。

3. 后处理方法

	一种明显降低SSE的方法是使用较大的K，然而很多情况下我们只想降低SSE，而不希望增加簇的个数，怎么办呢？一种常用的方法是交替的使用簇分裂和簇合并。

	增加簇个数的方法包括（分裂）：分裂一个具有较大SSE的簇或者分裂在特定属性上具有最大标准差的簇；引进一个新的质点，通常选择离所有簇质心最远的点，另一种方法是从所有的点或者具有最高SSE的点中随机选择。

	减少簇个数的方法包括（合并）：拆散一个簇，删除簇对应的质心，簇中的点重新指派到其他簇中；合并两个簇，通常选择质心距离最近的两个簇（质心法），或者选择合并后导致总SSE增加最少的簇（Ward法）。

4. 增量地更新质心

	可以在点到簇的每次指派之后，增量的更新质心，而不是在所有点都指派到簇中之后再更新。这样还可以保证不会产生空簇（因为所有的簇都是从单个点开始）。此外，如果使用增量更新还可以调整点的相对权值，增加准确率和收敛的速度，但是这样做通常相当困难（类似神经网络训练的权值）。缺点方面是，增量地更新质心肯能导致次序的依赖性，而且计算量更加复杂。

## 2.5 二分K均值

二分K均值的思想非常简单，为了得到K的簇，首先将所有的簇分成两个簇，然后从中选取一个簇进行二分裂，然后在从所有的簇中选择一个进行二分裂，知道生成K个簇。

待分裂簇的选择有许多种方法，一个是选择最大的簇，或者选择具有最大SSE的簇，或者选择一个基于大小和SSE的综合指标进行选择。K均值可以得到使总SSE局部最小的聚类，而二分K均值是不能保证的，因为它只是局部的使用了K均值算法。

前面也提到过了，二分K均值的有点就是不会受初始值选定的影响，而且速度更快。而且我们通常是使用已经聚类好的结果簇作为基本K均值的质心，对结果进行逐步求精。

## 2.6 K均值的适用簇类型

前面说过簇的类型分为很多种，K均值对于非球形形状或具有不同尺寸或密度的簇时，很难检测到“自然的”簇。K均值的目标函数是最小化等尺寸和等密度的球形簇，或者明显分离的簇，但是如果可以接受将一个自然簇分割成若干个子簇的话，这些局限性可以在某种意义上克服。

## 2.7 K-means优缺点

K均值简单而且可以用于各种类型的数据，也相当有效（毕竟是最古老最广泛使用的聚类方法），而且K均值的某些变种甚至更加有效。K均值不适合处理那些非球形簇、不同尺寸、不同密度的簇，而且对包含离群点的数据进行聚类是，K均值也是有问题的。


# 3.凝聚层次聚类

多元学过系统聚类，这就是我们所说的凝聚层次聚类。

层次聚类通常分为两种：凝聚的，从点作为个体簇开始，每一步合并两个最接近的簇，直到所有点合并成为一个簇；分裂的，从包含所有点的簇开始，每一步分裂一个簇，直到仅剩下单点簇。

这里先说凝聚层次聚类。凝聚层次聚类通常使用树状图（谱系图）显示，对于二维点的集合也可以使用嵌套图表示。

## 3.1 基本凝聚层次聚类算法

从个体点作为簇开始，一次合并两个最接近的簇，直到最后只剩下一个簇。

那么这里的关键就是如何定义两个簇之间的邻近度。系统聚类里面说了8种方法，这里简单的说一下其中三个最简单的：

1. 最长距离法（MAX，全链）利用两个簇中两个最远的点之间的邻近度作为簇的邻近度，即不同结点子集中两个结点之间的最长边。完全链接对于噪声点和离群点不太敏感，但是可能会使大的簇破裂，并且偏好球形簇(无法理解后两条，有高见的同学请帮理解一下)。

2. 最短距离法（MIN，单链）定义簇的邻近度为两个簇中距离最近的两个点之前的邻近度。单链技术擅长处理非椭圆形的簇，对噪声和离群点很敏感。

3. 平均距离法（组平均，group average）定义簇的邻近度为不同簇所有点对邻近度的平均值。

4. 此外，还有两种方法，是基于原型的观点，簇用质心代表，使用质心之间的邻近度作为簇的邻近度。这个叫做质心法，质心法看上去K均值相似，其实更相似的是下面的Ward法，质心法的一个特点是：倒置的可能性，即被合并的簇可能比前一步合并的簇更加相似，对于其他方法被合并的簇之间的邻近度随着层次聚类的进展单调的增加（或者不增加）。因此这种特性通常被视作一种缺点。

5. 另一种基于原型的方法是Ward方法，使用合并两个簇导致的SSE增加来度量两个簇之间的邻近度。该方法与K均值使用的目标函数相同，而且如果两个点之间的邻近度使用它们之间距离的平方计算时，Ward方法与组平均法十分相似。

## 3.2 优缺点

层次聚类可以产生层次结构，这可能正是某些数据所需要的结构，而且某些研究表明（...书中原话，感觉十分不靠谱），层次聚类可以产生高质量的聚类。

缺点来来来，一个个列出来：

1. 缺乏全局目标函数。层次聚类在每一步局部的使用一些标准来决定哪些簇应当合并，不能全局优化。但是这样做可以劈开解决困难的组合优化问题，而且解决了局部最小问题以及初始点的选择问题。

2. 合并是最终的。一旦两个簇进行合并，就不能撤消了。这种形式阻碍了局部最优变成全局最优。尽管如此，很多时候Ward还是作为一种初始化K均值聚类的鲁棒方法使用。解决这个问题的方法是：移动树的分支以改善全局目标函数，或者使用划分聚类技术（K均值）来创建许多小簇，然后从这些小簇出发进行层次聚类。

3. 计算量与存储量的代价是昂贵的。层次化聚类需要计算距离矩阵（期末考试手动计算...简直惨无人道），而且还要存储这个距离矩阵，所以面对数据量较大的情况，层次化聚类通常是乏力的。

4. 对于噪声、高维数据的处理能力差。因为所有合并都是最终的。

对于2、3、4问题都可以使用K均值进行部分聚类，在某种程度上可以解决这三个问题。

# 4.DBSCAN





