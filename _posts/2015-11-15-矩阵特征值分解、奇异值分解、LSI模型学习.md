---
layout: post
title: 矩阵特征值分解、奇异值分解学习
date: 2015-11-15
subtitle: 
author:     "Norris"
categories: blog
tags: [text mining,Statistic]
description: 舆情分析的项目要开始了啊...
---
# 一.特征值分解

学习SVD过程中，突然发现自己熟知的特征值特征向量概念模糊了起来，这个再熟悉不过的内容，在今天让我有了新的认识。

## 1.什么是矩阵的乘法？

矩阵的乘法是什么呢？看到这句话，学过线性代数的同学都会说出来是前一个矩阵的行乘以后一个矩阵的列，然而这又意味着什么呢？

变换。

矩阵乘法Y=AB就是线性变换。以其中一个向量A为中心，则B的作用就是使A发生：伸缩、切变、旋转的变换。

## 2.特征值、特征向量又意味着什么呢？

> 数学教材定义： 设A是n阶方阵，如果存在$\lambda$和n维非零向量X，使$Ax=\lambda x$,则$\lambda$称为方阵A的一个特征值，X为方阵A对应于或属于特征值$\lambda$的一个特征向量。

用矩阵乘法就是变换的含义来翻译一下这句话就是：

对特定的向量，经过一种方阵变换，经过该变换后，向量的方向不变（或只是反向），而只是进行伸缩变化（伸缩值可以是负值，相当于向量的方向反向）。

特征向量的代数上含义是：将矩阵乘法转换为数乘操作；特征向量的几何含义是：特征向量通过方阵A变换只进行伸缩，而保持特征向量的方向不变。特征值表示的是这个特征到底有多重要，类似于权重，而特征向量在几何上就是一个点，从原点到该点的方向表示向量的方向。

## 3.“特征值是震动的谱”

特征向量有一个重要的性质：同一特征值的任意多个特征向量的线性组合仍然是A属于同一特征值的特征向量。关于特征值，网上有一段关于“特征值是震动的谱”的解释：

>戏说在朝代宋的时候，我国就与发现矩阵特征值理论的机会擦肩而过。话说没有出息的秦少游在往池塘里扔了一颗小石头后，刚得到一句“投石冲开水底天”的泡妞诗对之后，就猴急猴急地去洞房了，全然没有想到水波中隐含着矩阵的特征值及特征向量的科学大道理。大概地说，水面附近的任一点水珠在原处上下振动（实际上在做近似圆周运动），并没有随着波浪向外圈移动，同时这些上下振动的水珠的幅度在渐渐变小，直至趋于平静。在由某块有着特定质量和形状的石头被以某种角度和速度投入某个面积和深度特定的水池中所决定的某个矩阵中，纹波荡漾中水珠的渐变过程中其特征值起着决定性的作用，它决定着水珠振动的频率和幅度减弱的衰退率。

>在理解关于振动的特征值和特征向量的过程中，需要加入复向量和复矩阵的概念，因为在实际应用中，实向量和实矩阵是干不了多少事的。机械振动和电振动有频谱，振动的某个频率具有某个幅度；那么矩阵也有矩阵的谱，矩阵的谱就是矩阵特征值的概念，是矩阵所固有的特性，所有的特征值形成了矩阵的一个频谱，每个特征值是矩阵的一个“谐振频点”。

美国数学家斯特让（G..Strang）在其经典教材《线性代数及其应用》中这样介绍了特征值作为频率的物理意义，他说：

>大概最简单的例子（我从不相信其真实性，虽然据说1831年有一桥梁毁于此因）是一对士兵通过桥梁的例子。传统上，他们要停止齐步前进而要散步通过。这个理由是因为他们可能以等于桥的特征值之一的频率齐步行进，从而将发生共振。就像孩子的秋千那样，你一旦注意到一个秋千的频率，和此频率相配，你就使频率荡得更高。一个工程师总是试图使他的桥梁或他的火箭的自然频率远离风的频率或液体燃料的频率；而在另一种极端情况，一个证券经纪人则尽毕生精力于努力到达市场的自然频率线。特征值是几乎任何一个动力系统的最重要的特征。

其实，这个矩阵之所以能形成“频率的谱”，就是因为矩阵在特征向量所指的方向上具有对向量产生恒定的变换作用：增强（或减弱）特征向量的作用。进一步的，如果矩阵持续地叠代作用于向量，那么特征向量的就会凸现出来。

## 4.特征值分解1.0:

一个变换方阵的所有特征向量组成了这个变换矩阵的一组基。所谓基，可以理解为坐标系的轴。我们平常用到的大多是直角坐标系，在线性代数中可以把这个坐标系扭曲、拉伸、旋转，称为基变换。我们可以按需求去设定基，但是基的轴之间必须是线性无关的，也就是保证坐标系的不同轴不要指向同一个方向或可以被别的轴组合而成，否则的话原来的空间就“撑”不起来了。从线性空间的角度看，在一个定义了内积的线性空间里，对一个N阶对称方阵进行特征分解，就是产生了该空间的N个标准正交基，然后把矩阵投影到这N个基上。N个特征向量就是N个标准正交基，而特征值的模则代表矩阵在每个基上的投影长度。特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。

在机器学习特征提取中，意思就是最大特征值对应的特征向量方向上包含最多的信息量，如果某几个特征值很小，说明这几个方向信息量很小，可以用来降维，也就是删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用信息量变化不大，PCA降维就是基于这种思路。

以上内容参考来源：<http://my.oschina.net/dfsj66011/blog/512198>

## 5.特征值分解2.0:

上面没有根本解释出矩阵乘法就是变换思维和特征值分解之间的联系，下面继续探究。

变换解释：

矩阵M如下：
$$M=\left(\begin{array}{cc}3&0\\0&1\end{array}\right)$$
它其实对应的线性变换是下面的形式：

![变换1](http://i12.tietuku.com/a8834661e3f14221.png)

因为这个矩阵M乘以一个向量(x,y)的结果是：
$$\left(\begin{array}{cc}3&0\\0&1\end{array}\right)\left(\begin{array}{c}x\\y\end{array}\right)=\left(\begin{array}{c}3x\\y\end{array}\right)$$

上面的矩阵是对称的，所以这个变换是一个对x，y轴的方向一个拉伸变换（每一个对角线上的元素将会对一个维度进行拉伸变换，当值大于1时，是拉长，当值小于1时时缩短），当矩阵不是对称的时候，假如说矩阵是下面的样子：
$$M=\left(\begin{array}{cc}1&1\\0&1\end{array}\right)$$

它所描述的变换是下面的样子：

![变换2](http://i12.tietuku.com/e72fbc41541ebb9b.png)

这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最主要的变化方向（变化方向可能有不止一个），如果我们想要描述好一个变换，那我们就描述好这个变换主要的变化方向就好了。反过头来看看之前特征值分解的式子，分解得到的Σ矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）


当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。也就是之前说的：提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。

# 二.奇异值分解：

## 1.奇异值分解概念

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个$$N\times M$$的矩阵就不可能是方阵.

我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法：
$$A=U\Sigma V^T$$

假设A是一个$$M\times N$$的矩阵，那么得到的U是一个$$M\times M$$的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），$$\Sigma$$是一个$$M\times N$$的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），$$V^T$$(V的转置)是一个$$N\times N$$的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量），反映几个相乘的矩阵大小的公式:

$$\operatorname*{A}_{m\times n}=\operatorname*{U}_{m\times m}\times \operatorname*{\Sigma}_{m\times n}\times \operatorname*{V^T}_{n\times n}$$

那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置乘以 A，将会得到一个方阵，我们用这个方阵求特征值可以得到：

$$\left(A^TA \right)v_i=\lambda_iv_i$$

这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：

$$ \sigma_i=\sqrt{\lambda_i}$$
$$ u_i=\frac{1}{\sigma_i}Av_i$$

这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，**前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。**也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：

$$A_{m\times n}\approx U_{m\times r}\Sigma_{r\times r}V^T_{r\times n}$$

r是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子：

$$\operatorname*{A}_{m\times n}=\operatorname*{U}_{m\times r}\times \operatorname*{\Sigma}_{r\times r}\times \operatorname*{V^T}_{r\times n}$$

右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵：U、Σ、V就好了。

## 2.奇异值的计算

奇异值的计算是一个难题，是一个O(N^3)的算法。在单机的情况下当然是没问题的，matlab在一秒钟内就可以算出1000 * 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。Google的吴军老师在数学之美系列谈到SVD的时候，说起Google实现了SVD的并行化算法，说这是对人类的一个贡献，但是也没有给出具体的计算规模，也没有给出太多有价值的信息。

其实SVD还是可以用并行的方式去实现的，在解大规模的矩阵的时候，一般使用迭代的方法，当矩阵的规模很大（比如说上亿）的时候，迭代的次数也可能会上亿次，如果使用Map-Reduce框架去解，则每次Map-Reduce完成的时候，都会涉及到写文件、读文件的操作。个人猜测Google云计算体系中除了Map-Reduce以外应该还有类似于MPI的计算模型，也就是节点之间是保持通信，数据是常驻在内存中的，这种计算模型比Map-Reduce在解决迭代次数非常多的时候，要快了很多倍。

Lanczos迭代就是一种解对称方阵部分特征值的方法（之前谈到了，解A’* A得到的对称方阵的特征值就是解A的右奇异向量），是将一个对称的方程化为一个三对角矩阵再进行求解。按网上的一些文献来看，Google应该是用这种方法去做的奇异值分解的。请见Wikipedia上面的一些引用的论文，如果理解了那些论文，也“几乎”可以做出一个SVD了。

由于奇异值的计算是一个很枯燥，纯数学的过程，而且前人的研究成果（论文中）几乎已经把整个程序的流程图给出来了。更多的关于奇异值计算的部分，将在后面的参考文献中给出，这里不再深入，我还是focus在奇异值的应用中去。

## 3.奇异值与主成分分析

 主成分分析在上一节里面也讲了一些，这里主要谈谈如何用SVD去解PCA的问题。PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子：

 ![PCA](http://i12.tietuku.com/4fab8270240b0a7c.png)

这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。

一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。

PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。

还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。

===================待续中

诸多思想来自于吴军老师的《数学之美》，在此安利大家可以拜读一下，通俗易懂，经典之作。


参考资料：<http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html>
