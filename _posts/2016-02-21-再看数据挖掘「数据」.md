---
layout: post
title: 再看数据挖掘--数据
subtitle: 本系列属原创 转载请注明原著
date: 2016-02-21
author:     "Norris"
categories: blog
tags: [数据挖掘]
---

此系列是看「数据挖掘导论」「统计学习方法」「ESL」等书的总结与自己的感悟。

# 1.概述

数据挖掘已然是当下做数据的人不能不知道的知识了，那么数据挖掘到底是什么呢。个人理解就是在数据中发现有价值的信息。数据挖掘的任务大概可以分为预测和描述两大类。预测包括我们常说的回归与分类，前者针对于目标变量是连续的情况，而后者针对目标变量是离散的情况。描述是做什么呢，就是概括数据中潜在的联系模式，比如相关性、趋势、聚类、异常等等。常用的技术包括分类、聚类、关联分析、异常检测等。

# 2.数据

说数据挖掘的技术之前，先得说说数据。巧妇难为无米之炊，数据对于数据分析人员的重要性不言而喻。罗老师讲过，数据分析就像做菜一样，首先得有材料（数据），然后还得洗菜（数据清洗、探索），然后得有菜谱（统计、机器学习模型），然后选择对的菜谱做菜（构建模型），最后摆摆盘（数据可视化，生成分析报告），再就是洗盘子了（规整资料）。作为基础，对于数据质量、类型的了解，对其处理的方式以及可用的模型是一个数据分析员具备的基本知识。

首先就是我们统计一开始就会学到的数据的类型：定类、定序、定距、定比。不多说了。而对于每种类型数据常用的方法可以归纳为：

类型|方法|变换方法
----|----|----
定类|众数，熵，列联相关|一对一变换
定序| 中位数、百分位数、秩相关、游程检验、符号检验|单调变换
定距|均值、方差、皮尔逊相关系数、t和F检验|正彷射变换
定比|几何平均数（环比）、调和平均数（相对变化率）、百分比变差|scalar

同时数据按照不同的分类标准也可以分为：连续数据、离散数据。对称数据、非对称数据（文档词频矩阵）。

一般在观察一个数据集的时候考虑三个方面的特性：维度（变量个数），稀疏性，分辨率。通过这三个方面来了解这个数据集的基本特性。

下面说说数据质量的问题，数据的质量关乎整个数据分析结构的优劣，数据的质量主要由这几个方面构成：

1. 测量误差和数据收集错误 
	
	测量误差在一定程度上是无法避免的，有可能是系统的也有可能是随机的。收集错误，一般都可以很好的纠正。

2. 噪声和伪像

	噪声是测量误差的随机部分，无法避免。一个算法的鲁棒性就是来衡量这个算法是否能应对噪声的干扰。数据错误或者确定性的失真就是伪像。

3. 精度、偏倚、准确率

4. 离群点 
	
	也称为异常值，离群点与噪声不同，是人们感兴趣的对象，如：信用卡欺诈。

5. 缺失值

	缺失值常用的处理方法： 删除数据或者属性；估计缺失值（众数、中位数、有分布的随机抽样）；忽略缺失值。

为了让数据更适合模型，为了改善模型、减少时间、降低成本、提高质量，常常需要对数据进行预处理。

1. 根据需要处理的数据规模，对碎片化的数据进行规整。例如我只需要年销售额的数据，而只有销售额的数据，就需要事先进行聚集。

2. 抽样。 选取具有代表性的样本。 常用的抽样方法有：简单随机抽样、分层抽样、整群抽样、有序抽样等等。

	确定正确的样本容量是一个麻烦事。可以采用渐进抽样的方法（可以理解为逐步试验），逐步增加样本容量，观察结果的准确性，当当本容量的增加使得结果准确定的增幅小于一个可以接受的阈值时，即可以停止，确定样本容量了。

3. 降维。Curse of Dimensionality听着名字就觉得太可怕了。降维也是近代统计研究的一个重要方向。在数据预处理的阶段我们就可以开始着手处理这个问题了。降低了维度首先会提高算法的性能，再者更容易理解（人对抽象空间的理解能力有限），而且可以方便与可视化。最最常用的线性技术就是大名鼎鼎的PCA了，还有他的好兄弟SVD。针对时间序列的数据，莫过于傅里叶变换了，以及小波变换。

	还有一种方法就要对数据的含义有很好的理解了，这就需要这个领域的专家才可以。比如人为的特征提取，处理一个问题，专家觉得这个问题的主要影响方面有哪几个就把哪几个变量作为输入。还有新特征的构造，也需要专家的指点，比如区分一堆金属，我们有他们的质量与体积，这两个变量不好利用，我们就创建一个新的变量：密度，这就是特征的构造。

4. 离散化。对于很多分类的模型，他们的目标变量往往是连续的，如何正确的将连续变量映射到离散空间也是有学问的。如果是非监督的方法，我们常用的就是分个区间，把对应的数标记为这个区间的类型，区间的划分主要有：等宽方法，等频率方法，以及K-means等聚类方法。监督的方法就可以使用熵来定义，先分成两部分，是的熵最小，取较大熵区间再分，循环往复。

	不仅仅连续变量要离散化，离散的变量有时候还需要二元化。（为什么是二元化呢？因为常见的模型都是这样简化的。比如决策树）这时候就能体现出进制的伟大了！（6个人60桶酒问题）不过这种情况下常常会出现共线性的问题（类似dummy variable的情况）。或者就直接使用dummy variable。

5. 根据自己经验的总结，感觉如果不是非常在乎度量的大小的话，尽量先标准化数据。（此条结果有待验证，有错误希望大家指出）

接下来再说说相似性、相异性的度量，为什么把这个也放在预处理这个阶段呢？因为许多算法例如聚类，KNN，异常检测等都需要相似相异的概念来衡量数据之间的亲疏远近，例如做系统聚类时，当计算出距离矩阵时就不需要原始数据了。如果不需要复杂的度量，可以参考下表：

属性类型|相异度|相似度
----|----|----
定类数据|dummy variable(相同取1，不同取0）|类似示性函数
定序|$$d=\frac{\lvert x-y \rvert}{n-1}$$|$$s=1-d$$
定距或定比| $$d=\lvert x-y \rvert $$|$$s=-d,s=\frac{1}{1+d},s=e^{-d},s=1-\frac{d-min\_d}{max\_d-min\_d}$$

面对复杂的相异度我们经常使用距离这个概念，常用的距离有欧几里得距离，其实是明可夫斯基距离的一种特殊形式，也是L2范数。还有常用的就是L1范数，上确界距离。还有包括马氏距离也比较常用。

复杂的相似度度量。如果数据是离散的可以使用混淆矩阵。基于混淆矩阵的概念有简单匹配系数（SMC），Jaccard系数（面对非对称的数据），以及余弦相似度（对象规范化，减少计算时间，非对称数据常用，量值重要时不要用）。面对连续的数据，可以使用常用的皮尔逊相关系数，其中可以选中中位数、绝对差、trim均值作为改进方法。

距离的选择还可以使用加权的方法，有点类似核函数的意思。

# 3.探索数据

数据清洗好了，先看看数据大概是个什么样子吧。这个阶段常用的技术就是汇总（计算均值方差），可视化（画个矩阵散点图看看相关性）还有不怎么明白的OLAP。

数据汇总学过统计概率论的人都十分明白啦，就说受可视化吧。

低位数据的可视化就是统计中常用的一些方法，非常好用。包括，茎叶图、直方图、箱线图、饼图、散点图等。

高维数据的可视化就不能仅仅使用坐标这个概念了，可以加入颜色、大小、形状等等来展示不同的维度。还有非常嘻哈的Chernoff脸！

OLAP不怎么懂，不多说了，大家可以自行查找文献。